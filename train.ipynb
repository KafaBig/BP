{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "id": "wAm09ZV7V7AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "import os\n",
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "tqdm.pandas()\n",
        "\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "NSoHVJ3gPy9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zkKOSoPPcsP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def display_dataset_statistics(data: List[Dict[str, Any]]):\n",
        "    print(f\"\\n {len(data)} examples loaded\")\n",
        "\n",
        "    question_lengths = [len(item['question']) for item in data]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(question_lengths, bins=30, edgecolor='black')\n",
        "    plt.title('Distribution of Question Lengths')\n",
        "    plt.xlabel('Question Length (characters)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n## Example entries:\")\n",
        "    for i in range(min(3, len(data))):\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(f\"Question: {data[i]['question']}\")\n",
        "        print(f\"Answer: {data[i]['answer']}\")\n",
        "        print(f\"Distractors: {data[i]['distractors']}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "def format_options(options_field: Any) -> List[str]:\n",
        "    if isinstance(options_field, str):\n",
        "        options_list = re.findall(r'[a-e]\\s*\\)\\s*([^,]*?)(?=\\s*$|\\s*,\\s*[a-e]\\s*\\))', options_field)\n",
        "        return [opt.strip() for opt in options_list] if options_list else []\n",
        "\n",
        "    if isinstance(options_field, list):\n",
        "        return [re.sub(r'^\\s*[a-e][\\):.-]?\\s*', '', opt.strip()) for opt in options_field]\n",
        "\n",
        "    raise ValueError(f\"Unexpected format in options: {options_field}\")\n",
        "\n",
        "\n",
        "def parse_options(options_list: List[str], correct_option: str) -> tuple:\n",
        "    try:\n",
        "        correct_index = ord(correct_option.lower()) - ord('a')\n",
        "        correct_answer = options_list[correct_index]\n",
        "        distractors = [opt for i, opt in enumerate(options_list) if i != correct_index]\n",
        "        return correct_answer, distractors\n",
        "    except (IndexError, ValueError):\n",
        "        raise ValueError(f\"Bad option '{correct_option}'\")\n",
        "\n",
        "def preprocess_data(\n",
        "    data: List[Dict[str, Any]],\n",
        "    max_distractors: int = 3,\n",
        "    print_examples: bool = False,\n",
        ") -> List[Dict[str, Any]]:\n",
        "\n",
        "    formatted_data = []\n",
        "    skipped_count = 0\n",
        "\n",
        "    if print_examples:\n",
        "        print(\"\\n 5 BEFORE processing: \")\n",
        "        for i in range(min(5, len(data))):\n",
        "          print(f\"Raw Example {i+1}:\")\n",
        "          print(json.dumps(data[i], indent=2))\n",
        "          print()\n",
        "\n",
        "    for item in tqdm(data, desc=f\"Processing data\"):\n",
        "        try:\n",
        "            if 'Problem' in item:  # MathQA\n",
        "                problem = item['Problem']\n",
        "                options_field = item.get('options', '')\n",
        "                correct = item.get('correct', '')\n",
        "\n",
        "                if not (problem and options_field and correct):\n",
        "                    skipped_count += 1\n",
        "                    continue\n",
        "\n",
        "                options = format_options(options_field)\n",
        "                correct_answer, distractors = parse_options(options, correct)\n",
        "\n",
        "            elif 'question' in item:  # MMLU-Pro\n",
        "                problem = item['question']\n",
        "                choices = item.get('options', [])\n",
        "                answer_idx = item.get('answer_index')\n",
        "\n",
        "                if not (problem and choices and answer_idx is not None):\n",
        "                    skipped_count += 1\n",
        "                    continue\n",
        "\n",
        "                correct_answer = choices[answer_idx]\n",
        "                distractors = [c for i, c in enumerate(choices) if i != answer_idx]\n",
        "\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            distractors = distractors[:max_distractors]\n",
        "            while len(distractors) < max_distractors:\n",
        "                distractors.append(\"No distractor available\")\n",
        "\n",
        "            formatted_entry = {\n",
        "                \"question\": problem,\n",
        "                \"answer\": correct_answer,\n",
        "                \"distractors\": distractors,\n",
        "            }\n",
        "            formatted_data.append(formatted_entry)\n",
        "\n",
        "        except Exception:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "    print(f\"Processed {len(formatted_data)}, skipped {skipped_count}\")\n",
        "\n",
        "    if print_examples:\n",
        "        print(\"\\n## 5 AFTER processing:\")\n",
        "        for i in range(min(5, len(formatted_data))):\n",
        "            print(f\"Processed Example {i+1}:\")\n",
        "            print(json.dumps(formatted_data[i], indent=2))\n",
        "            print()\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "def load_dataset_from_huggingface(dataset_name: str, split: str = \"train\"):\n",
        "    try:\n",
        "        dataset = load_dataset(dataset_name, split=split)\n",
        "        return [dict(item) for item in dataset]\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {dataset_name}: {e}\")\n",
        "        return []\n",
        "\n",
        "def process_and_save_dataset(\n",
        "    dataset_name: str,\n",
        "    output_path: Optional[str] = None,\n",
        "    max_distractors: int = 3,\n",
        "    num_samples: Optional[int] = None\n",
        ") -> Optional[List[Dict[str, Any]]]:\n",
        "\n",
        "    dataset_configs = {\n",
        "        \"allenai/math_qa\": {\"split\": \"train\", \"source\": \"mathqa\"},\n",
        "        \"TIGER-Lab/MMLU-Pro\": {\n",
        "            \"split\": \"test\",\n",
        "            \"source\": \"mmlu_pro\",\n",
        "            \"filter_categories\": [\"mathematics\", \"math\", \"arithmetic\", \"geometry\", \"algebra\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    config = dataset_configs.get(dataset_name, {})\n",
        "\n",
        "    data = load_dataset_from_huggingface(dataset_name, config.get('split', 'train'))\n",
        "\n",
        "    if dataset_name == \"TIGER-Lab/MMLU-Pro\" and config.get('filter_categories'):\n",
        "        data = [\n",
        "            item for item in data\n",
        "            if any(cat in str(item.get('category', '')).lower() for cat in config['filter_categories'])\n",
        "        ]\n",
        "\n",
        "    if num_samples:\n",
        "        data = data[:num_samples]\n",
        "\n",
        "    formatted_data = preprocess_data(\n",
        "        data,\n",
        "        max_distractors=max_distractors,\n",
        "        print_examples=True,\n",
        "    )\n",
        "\n",
        "    if output_path:\n",
        "        os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(formatted_data, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Dataset saved to {output_path}\")\n",
        "\n",
        "    display_dataset_statistics(formatted_data)\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "def combine_datasets(\n",
        "    datasets: List[str],\n",
        "    output_path: Optional[str] = None,\n",
        "    max_distractors: int = 3\n",
        ") -> List[Dict[str, Any]]:\n",
        "\n",
        "    combined_data = []\n",
        "    for dataset_name in datasets:\n",
        "        data = process_and_save_dataset(dataset_name, max_distractors=max_distractors)\n",
        "        combined_data.extend(data)\n",
        "\n",
        "    random.shuffle(combined_data)\n",
        "\n",
        "    if output_path:\n",
        "        os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(combined_data, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Combined dataset saved to {output_path}\")\n",
        "\n",
        "    display_dataset_statistics(combined_data)\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mathqa_data = process_and_save_dataset(\n",
        "        \"allenai/math_qa\",\n",
        "        output_path=\"./processed_data/mathqa_processed.json\",\n",
        "        max_distractors=3\n",
        "    )\n",
        "\n",
        "    mmlu_data = process_and_save_dataset(\n",
        "        \"TIGER-Lab/MMLU-Pro\",\n",
        "        output_path=\"./processed_data/mmlu_processed.json\",\n",
        "        max_distractors=3,\n",
        "        num_samples=1351\n",
        "    )\n",
        "\n",
        "    combined_data = combine_datasets(\n",
        "        [\"allenai/math_qa\", \"TIGER-Lab/MMLU-Pro\"],\n",
        "        output_path=\"./processed_data/combined_data.json\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "ePqxuLjMP-Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DistractorDatasetSeq2Seq(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.examples = []\n",
        "\n",
        "        special_tokens = {\"additional_special_tokens\": [\"<distractor1>\", \"<distractor2>\", \"<distractor3>\"]}\n",
        "        tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "        for item in data:\n",
        "            question = item[\"question\"]\n",
        "            answer = item[\"answer\"]\n",
        "            distractors = item[\"distractors\"]\n",
        "\n",
        "            input_text = f\"Question: {question} Answer: {answer}\"\n",
        "            target_text = f\"<distractor1> {distractors[0]} <distractor2> {distractors[1]} <distractor3> {distractors[2]}\"\n",
        "\n",
        "            input_encodings = self.tokenizer(input_text,\n",
        "                                     max_length=self.max_length,\n",
        "                                     padding=\"max_length\",\n",
        "                                     truncation=True,\n",
        "                                     return_tensors=\"pt\")\n",
        "\n",
        "            target_encodings = self.tokenizer(target_text,\n",
        "                                      max_length=self.max_length,\n",
        "                                      padding=\"max_length\",\n",
        "                                      truncation=True,\n",
        "                                      return_tensors=\"pt\")\n",
        "\n",
        "            self.examples.append({\n",
        "                \"input_ids\": input_encodings[\"input_ids\"][0],\n",
        "                \"attention_mask\": input_encodings[\"attention_mask\"][0],\n",
        "                \"labels\": target_encodings[\"input_ids\"][0],\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n"
      ],
      "metadata": {
        "id": "8P25bBsyQBY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DistractorDatasetCausalLM(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.examples = []\n",
        "\n",
        "\n",
        "        for item in data:\n",
        "            question = item[\"question\"]\n",
        "            answer = item[\"answer\"]\n",
        "            distractors = item[\"distractors\"]\n",
        "\n",
        "            formatted_input = f\"Question: {question} Answer: {answer} Distractors:\"\n",
        "            formatted_output = f\" {', '.join(distractors)}\"\n",
        "\n",
        "            full_text = formatted_input + formatted_output\n",
        "\n",
        "            encoded = self.tokenizer(full_text,\n",
        "                                     max_length=self.max_length,\n",
        "                                     padding=\"max_length\",\n",
        "                                     truncation=True,\n",
        "                                     return_tensors=\"pt\")\n",
        "\n",
        "            input_ids_len = len(self.tokenizer(formatted_input,\n",
        "                                              add_special_tokens=False)['input_ids'])\n",
        "\n",
        "            labels = encoded[\"input_ids\"].clone()\n",
        "            labels[0, :input_ids_len] = -100\n",
        "            attention_mask = encoded[\"attention_mask\"].clone()\n",
        "\n",
        "            self.examples.append({\n",
        "                \"input_ids\": encoded[\"input_ids\"][0],\n",
        "                \"attention_mask\": attention_mask[0],\n",
        "                \"labels\": labels[0]\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n"
      ],
      "metadata": {
        "id": "nTrcu0l4QBxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_distractor_model(data_path, test_data_path, model_name, output_dir, epochs=3, batch_size=4, lr=5e-5, gradient_accumulation_steps=1,\n",
        "                           weight_decay=0.01, warmup_ratio=0.1, lr_scheduler_type=\"linear\", early_stopping_patience=3):\n",
        "    data = load_data(data_path)\n",
        "\n",
        "    test_data = load_data(test_data_path)\n",
        "    train_data, eval_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model_name_lower = model_name.lower()\n",
        "    is_seq2seq = \"bart\" in model_name.lower() or \"t5\" in model_name.lower()\n",
        "\n",
        "    if is_seq2seq:\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        # Add special tokens for seq2seq\n",
        "        special_tokens = {\"additional_special_tokens\": [\"<distractor1>\", \"<distractor2>\", \"<distractor3>\"]}\n",
        "        num_added_tokens = tokenizer.add_special_tokens(special_tokens)\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "        train_dataset = DistractorDatasetSeq2Seq(train_data, tokenizer)\n",
        "        eval_dataset = DistractorDatasetSeq2Seq(eval_data, tokenizer)\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            if hasattr(model, 'resize_token_embeddings'):\n",
        "                model.resize_token_embeddings(len(tokenizer))\n",
        "        train_dataset = DistractorDatasetCausalLM(train_data, tokenizer)\n",
        "        eval_dataset = DistractorDatasetCausalLM(eval_data, tokenizer)\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        run_name=f\"{model_name.split('/')[-1]}-lr{lr}-bs{batch_size}-ep{epochs}\",\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        logging_steps=100,\n",
        "        learning_rate=lr,\n",
        "        weight_decay=weight_decay,\n",
        "        warmup_ratio=warmup_ratio,\n",
        "        lr_scheduler_type=lr_scheduler_type,\n",
        "        save_total_limit=2,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        report_to=\"wandb\",\n",
        "        disable_tqdm=False,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "    )\n",
        "\n",
        "    callbacks = []\n",
        "    if early_stopping_patience > 0:\n",
        "        early_stopping_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience\n",
        "        )\n",
        "        callbacks.append(early_stopping_callback)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    final_model_dir = f\"{output_dir}/final\"\n",
        "    model.save_pretrained(final_model_dir)\n",
        "    tokenizer.save_pretrained(final_model_dir)\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "    wandb.log({\"final_eval_loss\": eval_results[\"eval_loss\"]})\n",
        "\n",
        "    artifact = wandb.Artifact(name=f\"model-{wandb.run.id}\",\n",
        "                              type=\"model\",\n",
        "                              description=f\"trained model: {model_name}\")\n",
        "    artifact.add_dir(final_model_dir)\n",
        "    wandb.log_artifact(artifact)\n",
        "\n",
        "    print(f\"\\nEvaluating model on {len(test_data)} test examples...\")\n",
        "    evaluation_results = evaluate_model(model, tokenizer, is_seq2seq, test_data, f\"{model_name}-{wandb.run.id}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"avg_bert_score\": evaluation_results[\"avg_bert_score\"],\n",
        "        \"avg_rouge1_score\": evaluation_results[\"avg_rouge1_score\"],\n",
        "        \"avg_rouge2_score\": evaluation_results[\"avg_rouge2_score\"],\n",
        "        \"avg_rougeL_score\": evaluation_results[\"avg_rougeL_score\"]\n",
        "    })\n",
        "\n",
        "    examples_table = wandb.Table(columns=[\"Question\", \"Answer\", \"Gold Distractors\", \"Generated Distractors\", \"BERT Score\"])\n",
        "    for i in range(min(10, len(evaluation_results[\"results\"]))):\n",
        "        result = evaluation_results[\"results\"][i]\n",
        "        examples_table.add_data(\n",
        "            result[\"question\"],\n",
        "            result[\"answer\"],\n",
        "            str(result[\"gold_distractors\"]),\n",
        "            str(result[\"generated_distractors\"]),\n",
        "            result[\"bert_score\"]\n",
        "        )\n",
        "    wandb.log({\"evaluation_examples\": examples_table})\n",
        "\n",
        "    return evaluation_results\n"
      ],
      "metadata": {
        "id": "zL2KLL0aQIhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_bert_score(generated_distractors, gold_distractors):\n",
        "    if not generated_distractors or not gold_distractors:\n",
        "        print(\"Warning: Empty distractor list received\")\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        gen_distractors = [str(d) for d in generated_distractors if d]\n",
        "        gold_distractors = [str(d) for d in gold_distractors if d]\n",
        "\n",
        "        if not gen_distractors or not gold_distractors:\n",
        "            return 0.0\n",
        "\n",
        "        gen_embeddings = model.encode(gen_distractors)\n",
        "        gold_embeddings = model.encode(gold_distractors)\n",
        "\n",
        "        similarity_matrix = cosine_similarity(gen_embeddings, gold_embeddings)\n",
        "\n",
        "        max_similarities = np.max(similarity_matrix, axis=1)\n",
        "\n",
        "        return float(np.mean(max_similarities))\n",
        "    except Exception as e:\n",
        "        print(f\"BERT scoring fail: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def compute_rouge_scores(generated_distractors, gold_distractors):\n",
        "\n",
        "    try:\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "        gen_distractors = [str(d) for d in generated_distractors if d]\n",
        "        gold_distractors = [str(d) for d in gold_distractors if d]\n",
        "\n",
        "        if not gen_distractors or not gold_distractors:\n",
        "            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
        "\n",
        "        all_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "        for gen in gen_distractors:\n",
        "            best_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
        "\n",
        "            for gold in gold_distractors:\n",
        "                scores = scorer.score(gen, gold)\n",
        "\n",
        "                for metric in best_scores:\n",
        "                    if scores[metric].fmeasure > best_scores[metric]:\n",
        "                        best_scores[metric] = scores[metric].fmeasure\n",
        "\n",
        "            for metric in all_scores:\n",
        "                all_scores[metric].append(best_scores[metric])\n",
        "\n",
        "        return {\n",
        "            'rouge1': float(np.mean(all_scores['rouge1'])) if all_scores['rouge1'] else 0.0,\n",
        "            'rouge2': float(np.mean(all_scores['rouge2'])) if all_scores['rouge2'] else 0.0,\n",
        "            'rougeL': float(np.mean(all_scores['rougeL'])) if all_scores['rougeL'] else 0.0\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"ROUGE scoring failed: {e}\")\n",
        "        return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n"
      ],
      "metadata": {
        "id": "m5p7dW09FXlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_distractors(question, answer, model, tokenizer, is_seq2seq=False, num_distractors=3, max_length=100):\n",
        "\n",
        "    prompt = f\"Question: {question} Answer: {answer}\" if is_seq2seq else f\"Question: {question} Answer: {answer} Distractors:\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=min(len(input_ids[0]) + max_length, 512),\n",
        "            num_return_sequences=num_distractors*2,\n",
        "            num_beams=num_distractors*2,\n",
        "            temperature=0.8,\n",
        "            top_p=0.92,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=2,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    all_distractors = []\n",
        "    for seq in output:\n",
        "        generated_text = tokenizer.decode(seq, skip_special_tokens=True)\n",
        "\n",
        "        if is_seq2seq:\n",
        "            if \"<distractor1>\" in generated_text:\n",
        "                distractor_parts = generated_text.split(\"<distractor\")\n",
        "                for i in range(1, len(distractor_parts)):\n",
        "                    part = distractor_parts[i]\n",
        "                    if \">\" in part:\n",
        "                        distractor = part.split(\">\", 1)[1].strip()\n",
        "                        if \"<distractor\" in distractor:\n",
        "                            distractor = distractor.split(\"<distractor\")[0].strip()\n",
        "                        all_distractors.append(distractor)\n",
        "            else:\n",
        "                distractors_text = generated_text.strip()\n",
        "                if ',' in distractors_text:\n",
        "                    all_distractors.extend([d.strip() for d in distractors_text.split(',')])\n",
        "                else:\n",
        "                    potential_distractors = distractors_text.split()\n",
        "                    if all(d.replace('.', '').isdigit() for d in potential_distractors):\n",
        "                        all_distractors.extend(potential_distractors)\n",
        "                    else:\n",
        "                        all_distractors.append(distractors_text)\n",
        "        else:\n",
        "            if \"Distractors:\" in generated_text:\n",
        "                distractors_text = generated_text.split(\"Distractors:\")[1].strip()\n",
        "                if ',' in distractors_text:\n",
        "                    all_distractors.extend([d.strip() for d in distractors_text.split(',')])\n",
        "                else:\n",
        "                    potential_distractors = distractors_text.split()\n",
        "                    if all(d.replace('.', '').isdigit() for d in potential_distractors):\n",
        "                        all_distractors.extend(potential_distractors)\n",
        "                    else:\n",
        "                        sentences = re.split(r'[.!?]\\s+', distractors_text)\n",
        "                        if len(sentences) > 1:\n",
        "                            all_distractors.extend([s.strip() + '.' for s in sentences if s.strip()])\n",
        "                        else:\n",
        "                            all_distractors.append(distractors_text)\n",
        "\n",
        "    unique_distractors = []\n",
        "    seen = set()\n",
        "    answer_lower = answer.lower()\n",
        "\n",
        "    for distractor in all_distractors:\n",
        "        distractor_clean = distractor.strip()\n",
        "        if (distractor_clean and\n",
        "            distractor_clean not in seen and\n",
        "            distractor_clean.lower() != answer_lower):\n",
        "            seen.add(distractor_clean)\n",
        "            unique_distractors.append(distractor_clean)\n",
        "\n",
        "    if not unique_distractors and answer.replace('.', '').isdigit():\n",
        "        base = float(answer)\n",
        "        unique_distractors = [\n",
        "            str(round(base * 0.9, 2)),\n",
        "            str(round(base * 1.1, 2)),\n",
        "            str(round(base * 0.95, 2))\n",
        "        ]\n",
        "\n",
        "    return unique_distractors[:num_distractors]"
      ],
      "metadata": {
        "id": "qI_PSg_gQK9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_model(model, tokenizer, is_seq2seq, test_data, model_name):\n",
        "    results = []\n",
        "    bert_scores = []\n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "\n",
        "    print(f\"\\nEvaluating {model_name} on {len(test_data)} test examples...\")\n",
        "\n",
        "    for idx, item in enumerate(tqdm(test_data, desc=f\"Evaluating {model_name}\")):\n",
        "        question = item[\"question\"]\n",
        "        answer = item[\"answer\"]\n",
        "        gold_distractors = item[\"distractors\"]\n",
        "\n",
        "        #print(f\"\\nExample {idx+1}:\")\n",
        "        #print(f\"Question: {question}\")\n",
        "        #print(f\"Answer: {answer}\")\n",
        "        #print(f\"Gold distractors: {gold_distractors}\")\n",
        "\n",
        "        try:\n",
        "            generated_distractors = generate_distractors(\n",
        "                question, answer, model, tokenizer, is_seq2seq,\n",
        "                num_distractors=len(gold_distractors)\n",
        "            )\n",
        "\n",
        "            bert_score = compute_bert_score(generated_distractors, gold_distractors)\n",
        "            bert_scores.append(bert_score)\n",
        "\n",
        "            rouge_scores = compute_rouge_scores(generated_distractors, gold_distractors)\n",
        "            rouge1_scores.append(rouge_scores['rouge1'])\n",
        "            rouge2_scores.append(rouge_scores['rouge2'])\n",
        "            rougeL_scores.append(rouge_scores['rougeL'])\n",
        "\n",
        "            #print(f\"Generated distractors: {generated_distractors}\")\n",
        "            #print(f\"BERT score: {bert_score:.4f}\")\n",
        "            #print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "            #print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "            #print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
        "\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"gold_distractors\": gold_distractors,\n",
        "                \"generated_distractors\": generated_distractors,\n",
        "                \"bert_score\": bert_score,\n",
        "                \"rouge1_score\": rouge_scores['rouge1'],\n",
        "                \"rouge2_score\": rouge_scores['rouge2'],\n",
        "                \"rougeL_score\": rouge_scores['rougeL']\n",
        "            })\n",
        "\n",
        "            if (idx + 1) % 10 == 0:\n",
        "                interim_bert = np.mean(bert_scores[-10:])\n",
        "                interim_rouge1 = np.mean(rouge1_scores[-10:])\n",
        "                print(f\"Last 10 examples - BERT: {interim_bert:.4f}, ROUGE-1: {interim_rouge1:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"skipped example {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    avg_bert = np.mean(bert_scores) if bert_scores else 0\n",
        "    avg_rouge1 = np.mean(rouge1_scores) if rouge1_scores else 0\n",
        "    avg_rouge2 = np.mean(rouge2_scores) if rouge2_scores else 0\n",
        "    avg_rougeL = np.mean(rougeL_scores) if rougeL_scores else 0\n",
        "\n",
        "    print(\"\\n==== EVALUATION SUMMARY ====\")\n",
        "    print(f\"Total examples evaluated: {len(results)}\")\n",
        "    print(f\"Average BERT score: {avg_bert:.4f}\")\n",
        "    print(f\"Average ROUGE-1 score: {avg_rouge1:.4f}\")\n",
        "    print(f\"Average ROUGE-2 score: {avg_rouge2:.4f}\")\n",
        "    print(f\"Average ROUGE-L score: {avg_rougeL:.4f}\")\n",
        "\n",
        "    summary = {\n",
        "        \"model_name\": model_name,\n",
        "        \"avg_bert_score\": avg_bert,\n",
        "        \"avg_rouge1_score\": avg_rouge1,\n",
        "        \"avg_rouge2_score\": avg_rouge2,\n",
        "        \"avg_rougeL_score\": avg_rougeL,\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "a8Mg0ccQGZRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_sweep_config():\n",
        "    sweep_config = {\n",
        "        'method': 'bayes',\n",
        "        'metric': {'name': 'avg_bert_score','goal': 'maximize'},\n",
        "        'parameters': {\n",
        "            'learning_rate': {'values': [1e-6, 1e-5, 5e-5, 1e-4]},\n",
        "            'epochs': {'values': [3,4,5,6,7,8,9]},\n",
        "            'batch_size': {'values': [8, 16, 32]},\n",
        "            'model_name': {'values': ['facebook/bart-base','t5-small','gpt2']},\n",
        "            'data_path': {'values': ['processed_data/combined_data.json']},\n",
        "            'test_data_path': {'values': ['test_data_dir/arithmetik_questions.json']},\n",
        "            'model_output_dir': {'values': ['distractor_model']},\n",
        "            'gradient_accumulation_steps': {'values': [2, 4]},\n",
        "            'weight_decay': {'values': [0.01, 0.05, 0.1]},\n",
        "            'warmup_ratio': {'values': [0.05, 0.1, 0.15]},\n",
        "            'lr_scheduler_type': {'values': ['linear', 'cosine', 'cosine_with_restarts']},\n",
        "            'early_stopping_patience': {'values': [3, 5]}\n",
        "        }\n",
        "    }\n",
        "    return sweep_config\n",
        "\n"
      ],
      "metadata": {
        "id": "t3Dy8xrGUCam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_evaluate_sweep():\n",
        "    wandb.init()\n",
        "\n",
        "    config = wandb.config\n",
        "\n",
        "    train_data_path = config.data_path\n",
        "    test_data_path = config.test_data_path\n",
        "    model_name = config.model_name\n",
        "    output_dir = config.model_output_dir\n",
        "    epochs = config.epochs\n",
        "    batch_size = config.batch_size\n",
        "    lr = config.learning_rate\n",
        "    gradient_accumulation_steps = config.gradient_accumulation_steps\n",
        "    weight_decay = config.weight_decay\n",
        "    warmup_ratio = config.warmup_ratio\n",
        "    lr_scheduler_type = config.lr_scheduler_type\n",
        "    early_stopping_patience = config.early_stopping_patience\n",
        "\n",
        "    evaluation_results = train_distractor_model(\n",
        "        train_data_path,\n",
        "        test_data_path,\n",
        "        model_name,\n",
        "        output_dir,\n",
        "        epochs,\n",
        "        batch_size,\n",
        "        lr,\n",
        "        gradient_accumulation_steps,\n",
        "        weight_decay,\n",
        "        warmup_ratio,\n",
        "        lr_scheduler_type,\n",
        "        early_stopping_patience\n",
        "    )\n",
        "\n",
        "    print(f\"Training and evaluation complete for {model_name}\")\n",
        "    print(f\"BERT Score: {evaluation_results['avg_bert_score']:.4f}\")\n",
        "    print(f\"ROUGE-1: {evaluation_results['avg_rouge1_score']:.4f}\")\n",
        "    print(f\"ROUGE-2: {evaluation_results['avg_rouge2_score']:.4f}\")\n",
        "    print(f\"ROUGE-L: {evaluation_results['avg_rougeL_score']:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jqVS_NMVUHon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    wandb_key = userdata.get('WANDB_KEY')\n",
        "    wandb.login(key=wandb_key)\n",
        "\n",
        "    sweep_config = create_sweep_config()\n",
        "\n",
        "    sweep_id = wandb.sweep(sweep_config, project=\"bachelorprojekt\")\n",
        "\n",
        "    wandb.agent(sweep_id, train_evaluate_sweep, count=10)\n"
      ],
      "metadata": {
        "id": "nw6cC4wFFwxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "GaCmnnebF1gX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}